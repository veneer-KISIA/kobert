{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "주의점\n",
    "1. 문장이 끊겨서 나눠져있음 (일상대화 데이터)<br>\n",
    "`<SEP>` 토큰 관련\n",
    "\n",
    "2. 라벨과 토큰 분리 관련<br>\n",
    "봉구스 밥버거라는 --> '봉구스 밥버거' 로 라벨링되어있음\n",
    "\n",
    "3. 비식별화 된 데이터 처리\n",
    "%company name, name etc\n",
    "\n",
    "\n",
    "\"SDRW2000000026.1.1.97\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projects\\AISec\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, DistilBertModel\n",
    "bert_model = BertModel.from_pretrained('monologg/kobert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'KoBertTokenizer'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['[CLS]', '▁한국', '어', '▁모델', '을', '▁공유', '합니다', '.', '[SEP]']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenization_kobert import KoBertTokenizer\n",
    "tokenizer = KoBertTokenizer.from_pretrained('monologg/kobert') # monologg/distilkobert도 동일\n",
    "tokenizer.tokenize(\"[CLS] 한국어 모델을 공유합니다. [SEP]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁봉', '구', '스', '▁밥', '버', '거', '라는', '▁곳', '에서', '▁근무', '를', '▁했', '었', '을', '▁때', '에도', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.tokenize(\"봉구스 밥버거라는 곳에서 근무를 했었을 때에도. [SEP]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('./NIKL_NE_2022_v1.0/SXNE2202211218.json', 'r', encoding='utf-8') as f:\n",
    "    test = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'SDRW2000000006.1.1.6',\n",
       "  'form': '그래서 떡볶이',\n",
       "  'word': [{'id': 1, 'form': '그래서', 'begin': 0, 'end': 3},\n",
       "   {'id': 2, 'form': '떡볶이', 'begin': 4, 'end': 7}],\n",
       "  'NE': [{'id': 1, 'form': '떡볶이', 'label': 'CV_FOOD', 'begin': 4, 'end': 7}]},\n",
       " {'id': 'SDRW2000000006.1.1.7',\n",
       "  'form': '좋아하고',\n",
       "  'word': [{'id': 1, 'form': '좋아하고', 'begin': 0, 'end': 4}],\n",
       "  'NE': []},\n",
       " {'id': 'SDRW2000000006.1.1.8',\n",
       "  'form': '카레도',\n",
       "  'word': [{'id': 1, 'form': '카레도', 'begin': 0, 'end': 3}],\n",
       "  'NE': [{'id': 1, 'form': '카레', 'label': 'CV_FOOD', 'begin': 0, 'end': 2}]},\n",
       " {'id': 'SDRW2000000006.1.1.9',\n",
       "  'form': '매운 카레를 좋아하고',\n",
       "  'word': [{'id': 1, 'form': '매운', 'begin': 0, 'end': 2},\n",
       "   {'id': 2, 'form': '카레를', 'begin': 3, 'end': 6},\n",
       "   {'id': 3, 'form': '좋아하고', 'begin': 7, 'end': 11}],\n",
       "  'NE': [{'id': 1, 'form': '카레', 'label': 'CV_FOOD', 'begin': 3, 'end': 5}]},\n",
       " {'id': 'SDRW2000000006.1.1.10',\n",
       "  'form': '돈가스를 먹어도',\n",
       "  'word': [{'id': 1, 'form': '돈가스를', 'begin': 0, 'end': 4},\n",
       "   {'id': 2, 'form': '먹어도', 'begin': 5, 'end': 8}],\n",
       "  'NE': [{'id': 1, 'form': '돈가스', 'label': 'CV_FOOD', 'begin': 0, 'end': 3}]},\n",
       " {'id': 'SDRW2000000006.1.1.11',\n",
       "  'form': '매운 돈가스를 좋아합니다.',\n",
       "  'word': [{'id': 1, 'form': '매운', 'begin': 0, 'end': 2},\n",
       "   {'id': 2, 'form': '돈가스를', 'begin': 3, 'end': 7},\n",
       "   {'id': 3, 'form': '좋아합니다.', 'begin': 8, 'end': 14}],\n",
       "  'NE': [{'id': 1, 'form': '돈가스', 'label': 'CV_FOOD', 'begin': 3, 'end': 6}]}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['document'][0]['sentence'][5:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "그래서 떡볶이 좋아하고 카레도 매운 카레를 좋아하고 돈가스를 먹어도 매운 돈가스를 좋아합니다. B-CV_FOOD I-CV_FOOD O B-CV_FOOD B-CV_FOOD I-CV_FOOD O B-CV_FOOD O B-CV_FOOD I-CV_FOOD O\n"
     ]
    }
   ],
   "source": [
    "from io import StringIO\n",
    "\n",
    "sentences = []\n",
    "labels = []\n",
    "doc_sentences = test['document'][0]['sentence'][5:11]\n",
    "\n",
    "sent = StringIO()\n",
    "label = []\n",
    "for words in doc_sentences:\n",
    "    # sentence\n",
    "    sent.write(words['form'])\n",
    "\n",
    "    # label\n",
    "    ne = words['NE']\n",
    "    words_length = len(words['word'])\n",
    "    if not ne:\n",
    "        label.extend(['O'] * words_length)  # add O label\n",
    "    else:\n",
    "        ne_iter = iter(ne)\n",
    "        next_label = next(ne_iter)\n",
    "        try:\n",
    "            for i, word in enumerate(words['word'], 1):\n",
    "                word_beg = word['begin']\n",
    "                word_end = word['end']\n",
    "\n",
    "                if word_beg <= next_label['begin'] and next_label['end'] <= word_end:  # label word\n",
    "                    if word_beg == next_label['begin']:  # start of label\n",
    "                        label.append(f\"B-{next_label['label']}\")\n",
    "\n",
    "                    if word_beg < next_label['begin']:  # label is multiple words\n",
    "                        label.append(f\"I-{next_label['label']}\")\n",
    "\n",
    "                    if next_label['end'] <= word['end']:  # end of label\n",
    "                        next_label = next(ne_iter)     \n",
    "                \n",
    "                else:\n",
    "                    label.append('O')\n",
    "\n",
    "        except StopIteration:\n",
    "            label.extend(['O'] * (words_length - i))                \n",
    "\n",
    "    # end of sentence\n",
    "    if words['form'].endswith('.') or words['form'].endswith('?'):\n",
    "        sentences.append(sent.getvalue())\n",
    "        labels.append(' '.join(label))\n",
    "        sent = StringIO()  # new sentence\n",
    "        label = []\n",
    "        \n",
    "    else:\n",
    "        sent.write(' ')\n",
    "\n",
    "for s, l in zip(sentences, labels):\n",
    "    print(s, l)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
