{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "주의점\n",
    "1. 문장이 끊겨서 나눠져있음 (일상대화 데이터)<br>\n",
    "`<SEP>` 토큰 관련\n",
    "\n",
    "2. 라벨과 토큰 분리 관련<br>\n",
    "봉구스 밥버거라는 --> '봉구스 밥버거' 로 라벨링되어있음\n",
    "\n",
    "3. 비식별화 된 데이터 처리\n",
    "%company name, name etc\n",
    "\n",
    "\n",
    "오류\n",
    "\n",
    "\n",
    "\"id\": \"SDRW2000000179.1.1.87\",\n",
    "\"form\": \"콜라겐을 챙겨 먹으려고 | 하는데\"\n",
    "\n",
    "| -> 토큰 개수 때문에 제거\n",
    "\n",
    "\n",
    "\"id\": \"SDRW2000000231.1.1.342\",\n",
    "\"form\": \"연기를 조금 해보고 싶어 .\"\n",
    "\n",
    ". -> 토큰 취급 (띄어쓰기)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projects\\AISec\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, DistilBertModel\n",
    "bert_model = BertModel.from_pretrained('monologg/kobert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'KoBertTokenizer'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['[CLS]', '▁한국', '어', '▁모델', '을', '▁공유', '합니다', '.', '[SEP]']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenization_kobert import KoBertTokenizer\n",
    "tokenizer = KoBertTokenizer.from_pretrained('monologg/kobert') # monologg/distilkobert도 동일\n",
    "tokenizer.tokenize(\"[CLS] 한국어 모델을 공유합니다. [SEP]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁봉', '구', '스', '▁밥', '버', '거', '라는', '▁곳', '에서', '▁근무', '를', '▁했', '었', '을', '▁때', '에도', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.tokenize(\"봉구스 밥버거라는 곳에서 근무를 했었을 때에도. [SEP]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('./NIKL_NE_2022_v1.0/SXNE2202211218.json', 'r', encoding='utf-8') as f:\n",
    "    test = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'SDRW2000000006.1.1.6',\n",
       "  'form': '그래서 떡볶이',\n",
       "  'word': [{'id': 1, 'form': '그래서', 'begin': 0, 'end': 3},\n",
       "   {'id': 2, 'form': '떡볶이', 'begin': 4, 'end': 7}],\n",
       "  'NE': [{'id': 1, 'form': '떡볶이', 'label': 'CV_FOOD', 'begin': 4, 'end': 7}]},\n",
       " {'id': 'SDRW2000000006.1.1.7',\n",
       "  'form': '좋아하고',\n",
       "  'word': [{'id': 1, 'form': '좋아하고', 'begin': 0, 'end': 4}],\n",
       "  'NE': []},\n",
       " {'id': 'SDRW2000000006.1.1.8',\n",
       "  'form': '카레도',\n",
       "  'word': [{'id': 1, 'form': '카레도', 'begin': 0, 'end': 3}],\n",
       "  'NE': [{'id': 1, 'form': '카레', 'label': 'CV_FOOD', 'begin': 0, 'end': 2}]},\n",
       " {'id': 'SDRW2000000006.1.1.9',\n",
       "  'form': '매운 카레를 좋아하고',\n",
       "  'word': [{'id': 1, 'form': '매운', 'begin': 0, 'end': 2},\n",
       "   {'id': 2, 'form': '카레를', 'begin': 3, 'end': 6},\n",
       "   {'id': 3, 'form': '좋아하고', 'begin': 7, 'end': 11}],\n",
       "  'NE': [{'id': 1, 'form': '카레', 'label': 'CV_FOOD', 'begin': 3, 'end': 5}]},\n",
       " {'id': 'SDRW2000000006.1.1.10',\n",
       "  'form': '돈가스를 먹어도',\n",
       "  'word': [{'id': 1, 'form': '돈가스를', 'begin': 0, 'end': 4},\n",
       "   {'id': 2, 'form': '먹어도', 'begin': 5, 'end': 8}],\n",
       "  'NE': [{'id': 1, 'form': '돈가스', 'label': 'CV_FOOD', 'begin': 0, 'end': 3}]},\n",
       " {'id': 'SDRW2000000006.1.1.11',\n",
       "  'form': '매운 돈가스를 좋아합니다.',\n",
       "  'word': [{'id': 1, 'form': '매운', 'begin': 0, 'end': 2},\n",
       "   {'id': 2, 'form': '돈가스를', 'begin': 3, 'end': 7},\n",
       "   {'id': 3, 'form': '좋아합니다.', 'begin': 8, 'end': 14}],\n",
       "  'NE': [{'id': 1, 'form': '돈가스', 'label': 'CV_FOOD', 'begin': 3, 'end': 6}]}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['document'][0]['sentence'][5:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DT_YEAR-I', 'QT_OTHERS-I', 'CV_PRIZE-I', 'LCP_COUNTY-I', 'TMI_SERVICE-I', 'PS_CHARACTER-B', 'OGG_POLITICS-I', 'EV_SPORTS-I', 'CV_SPORTS_INST-B', 'LCG_OCEAN-B', 'OGG_ECONOMY-B', 'AM_PART-B', 'DT_WEEK-I', 'MT_ELEMENT-B', 'OGG_HOTEL-B', 'QT_PHONE-B', 'QT_SIZE-I', 'QT_PERCENTAGE-B', 'TM_DIRECTION-I', 'AFA_MUSIC-B', 'QT_WEIGHT-B', 'LCG_MOUNTAIN-B', 'CV_DRINK-B', 'PS_CHARACTER-I', 'CV_LANGUAGE-B', 'PS_NAME-I', 'TI_OTHERS-B', 'CV_SPORTS_POSITION-I', 'OGG_SCIENCE-B', 'EV_ACTIVITY-I', 'EV_WAR_REVOLUTION-I', 'QT_AGE-B', 'OGG_FOOD-B', 'EV_FESTIVAL-I', 'AM_MAMMALIA-B', 'AF_CULTURAL_ASSET-B', 'AFA_DOCUMENT-B', 'DT_DURATION-I', 'OGG_LAW-B', 'TI_OTHERS-I', 'TM_CELL_TISSUE_ORGAN-I', 'DT_DURATION-B', 'DT_WEEK-B', 'AFA_ART_CRAFT-B', 'AFW_SERVICE_PRODUCTS-I', 'DT_YEAR-B', 'CV_ART-I', 'OGG_MILITARY-B', 'OGG_HOTEL-I', 'CV_POLICY-B', 'EV_FESTIVAL-B', 'FD_HUMANITIES-I', 'AF_MUSICAL_INSTRUMENT-B', 'DT_DYNASTY-B', 'PT_FRUIT-B', 'OGG_RELIGION-B', 'LCG_BAY-B', 'QT_COUNT-B', 'OGG_FOOD-I', 'TI_HOUR-B', 'TM_CELL_TISSUE_ORGAN-B', 'CV_FOOD_STYLE-I', 'AM_OTHERS-B', 'CV_TRIBE-I', 'QT_COUNT-I', 'CV_SPORTS_POSITION-B', 'CV_RELATION-I', 'TR_ART-B', 'TMI_HW-B', 'TMIG_GENRE-B', 'CV_FOOD-B', 'OGG_SPORTS-I', 'LCG_MOUNTAIN-I', 'AM_REPTILIA-B', 'AFA_PERFORMANCE-I', 'CV_BUILDING_TYPE-B', 'TM_COLOR-I', 'AM_BIRD-B', 'CV_POSITION-I', 'PT_OTHERS-B', 'CV_FOOD-I', 'LCG_ISLAND-B', 'PT_GRASS-I', 'LC_SPACE-B', 'FD_MEDICINE-B', 'QT_PRICE-I', 'AFA_VIDEO-B', 'CV_LAW-I', 'AFA_DOCUMENT-I', 'MT_ROCK-B', 'DT_OTHERS-I', 'OGG_MEDIA-I', 'CV_POLICY-I', 'AF_WEAPON-I', 'LCP_COUNTRY-I', 'CV_FOOD_STYLE-B', 'TR_SOCIAL_SCIENCE-I', 'TM_CLIMATE-B', 'OGG_RELIGION-I', 'AFA_ART_CRAFT-I', 'OGG_MILITARY-I', 'LCP_PROVINCE-B', 'QT_TEMPERATURE-B', 'FD_MEDICINE-I', 'QT_OTHERS-B', 'AF_ROAD-B', 'QT_MAN_COUNT-B', 'EV_WAR_REVOLUTION-B', 'LC_OTHERS-B', 'TM_CLIMATE-I', 'FD_ART-I', 'CV_TAX-B', 'CV_DRINK-I', 'QT_SPORTS-I', 'CV_PRIZE-B', 'PT_FLOWER-I', 'CV_RELATION-B', 'AFW_SERVICE_PRODUCTS-B', 'TM_SPORTS-I', 'O', 'DT_OTHERS-B', 'AFA_MUSIC-I', 'TI_HOUR-I', 'TR_MEDICINE-B', 'CV_CLOTHING-B', 'QT_TEMPERATURE-I', 'CV_CULTURE-I', 'TMM_DRUG-I', 'QT_LENGTH-B', 'AM_FISH-B', 'OGG_MEDICINE-I', 'TMM_DRUG-B', 'CV_LAW-B', 'PT_GRASS-B', 'TR_HUMANITIES-B', 'QT_PRICE-B', 'CV_ART-B', 'QT_WEIGHT-I', 'LCP_CITY-B', 'LCP_PROVINCE-I', 'AF_BUILDING-I', 'TM_COLOR-B', 'QT_SPEED-B', 'TMI_PROJECT-B', 'DT_MONTH-B', 'OGG_MEDICINE-B', 'OGG_LIBRARY-B', 'PS_NAME-B', 'LCP_COUNTY-B', 'AM_MAMMALIA-I', 'OGG_POLITICS-B', 'AF_BUILDING-B', 'CV_TAX-I', 'UNK', 'EV_OTHERS-I', 'TMI_HW-I', 'AM_AMPHIBIA-B', 'FD_HUMANITIES-B', 'CV_SPORTS-I', 'OGG_EDUCATION-I', 'OGG_OTHERS-B', 'FD_SOCIAL_SCIENCE-B', 'MT_CHEMICAL-I', 'TM_SHAPE-B', 'OGG_ART-B', 'TR_OTHERS-B', 'TR_SOCIAL_SCIENCE-B', 'CV_TRIBE-B', 'EV_SPORTS-B', 'AFA_VIDEO-I', 'TR_ART-I', 'PT_FLOWER-B', 'PT_TYPE-B', 'PT_PART-B', 'AFW_OTHER_PRODUCTS-I', 'TI_DURATION-B', 'TI_MINUTE-B', 'AM_INSECT-B', 'EV_ACTIVITY-B', 'PT_FRUIT-I', 'TM_DIRECTION-B', 'DT_DAY-B', 'TM_SPORTS-B', 'PT_TREE-B', 'LCG_CONTINENT-B', 'AM_PART-I', 'LCP_CAPITALCITY-B', 'OGG_EDUCATION-B', 'QT_VOLUME-I', 'QT_ALBUM-B', 'MT_METAL-B', 'QT_SPEED-I', 'QT_AGE-I', 'QT_VOLUME-B', 'QT_PERCENTAGE-I', 'TR_SCIENCE-B', 'AFA_PERFORMANCE-B', 'AF_ROAD-I', 'CV_OCCUPATION-I', 'OGG_OTHERS-I', 'FD_SCIENCE-B', 'CV_CULTURE-B', 'OGG_ART-I', 'DT_DAY-I', 'CV_POSITION-B', 'TR_SCIENCE-I', 'PS_PET-B', 'CV_CLOTHING-I', 'AF_TRANSPORT-B', 'AM_TYPE-B', 'TMI_SERVICE-B', 'DT_SEASON-I', 'PT_OTHERS-I', 'DT_SEASON-B', 'TI_DURATION-I', 'OGG_SPORTS-B', 'QT_SPORTS-B', 'TMM_DISEASE-I', 'AF_WEAPON-B', 'QT_SIZE-B', 'OGG_ECONOMY-I', 'AM_OTHERS-I', 'OGG_MEDIA-B', 'FD_SCIENCE-I', 'QT_ALBUM-I', 'TMM_DISEASE-B', 'DT_MONTH-I', 'AF_TRANSPORT-I', 'MT_CHEMICAL-B', 'LC_OTHERS-I', 'LCG_RIVER-B', 'EV_OTHERS-B', 'CV_SPORTS_INST-I', 'LCG_RIVER-I', 'DT_DYNASTY-I', 'QT_ORDER-I', 'DT_GEOAGE-B', 'CV_FUNDS-B', 'FD_SOCIAL_SCIENCE-I', 'AF_CULTURAL_ASSET-I', 'FD_OTHERS-B', 'CV_OCCUPATION-B', 'FD_ART-B', 'AFW_OTHER_PRODUCTS-B', 'LCP_COUNTRY-B', 'CV_SPORTS-B', 'CV_FUNDS-I', 'QT_LENGTH-I', 'CV_CURRENCY-B', 'TMI_SW-B', 'TMIG_GENRE-I', 'TR_MEDICINE-I', 'TMI_PROJECT-I', 'QT_ORDER-B', 'QT_MAN_COUNT-I', 'TM_SHAPE-I'}\n"
     ]
    }
   ],
   "source": [
    "from io import StringIO\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "label_set = set(['UNK'])\n",
    "sentences = []\n",
    "labels = []\n",
    "\n",
    "for document in test['document']:\n",
    "    doc_sentences = document['sentence']\n",
    "\n",
    "    sent = StringIO()\n",
    "    label = []\n",
    "    for words in doc_sentences:\n",
    "        # sentence\n",
    "        sent.write(words['form'].replace('|', ''))\n",
    "\n",
    "        # label\n",
    "        ne = words['NE']\n",
    "        words_length = len(words['word'])\n",
    "        if not ne:\n",
    "            label.extend(['O'] * words_length)  # add O label\n",
    "        else:\n",
    "            ne_iter = iter(ne)\n",
    "            next_label = next(ne_iter)\n",
    "            try:\n",
    "                for i, word in enumerate(words['word'], 1):\n",
    "                    word_beg = word['begin']\n",
    "                    word_end = word['end']\n",
    "\n",
    "                    if next_label['begin'] <= word_end:  # reached label word\n",
    "                        if word_beg <= next_label['begin']:  # if word start of label\n",
    "                            label.append(f\"{next_label['label']}-B\")\n",
    "\n",
    "                        else:  # label is multiple words, but does not contain begining\n",
    "                            label.append(f\"{next_label['label']}-I\")\n",
    "\n",
    "                        if next_label['end'] <= word_end:  # end of label\n",
    "                            next_label = next(ne_iter)     \n",
    "                    \n",
    "                    else:\n",
    "                        label.append('O')\n",
    "\n",
    "            except StopIteration:\n",
    "                label.extend(['O'] * (words_length - i))                \n",
    "\n",
    "        # end of sentence\n",
    "        if words['form'].endswith('.') or words['form'].endswith('?'):\n",
    "            # separate . or ?\n",
    "            # sent.seek(sent.tell()-1)\n",
    "            # sent.write(f\" {words['form'][-1]}\")\n",
    "            # label.append('O')\n",
    "\n",
    "            label_set.update(label)\n",
    "\n",
    "            sentences.append(sent.getvalue())\n",
    "            labels.append(' '.join(label))\n",
    "            sent = StringIO()  # new sentence\n",
    "            label = []\n",
    "            \n",
    "        else:\n",
    "            sent.write(' ')\n",
    "\n",
    "with open('./data/train.tsv', 'w', encoding='utf-8') as f_train, open('./data/test.tsv', 'w', encoding='utf-8') as f_test:\n",
    "\n",
    "    for s, l in zip(sentences, labels):\n",
    "        rand = random.random()\n",
    "        if rand < 0.8:\n",
    "            f_train.write(f'{s}\\t{l}\\n')\n",
    "        else:\n",
    "            f_test.write(f'{s}\\t{l}\\n')\n",
    "\n",
    "print(label_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = list(label_set)\n",
    "label_list.sort()\n",
    "\n",
    "with open('./data/label.txt', 'w', encoding='utf-8') as f_label:\n",
    "    for lab in label_list:\n",
    "        f_label.write(f'{lab}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test for cuda\n",
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'KoBertTokenizer'.\n",
      "Predicting: 100%|██████████| 1/1 [00:00<00:00,  5.24it/s]\n"
     ]
    }
   ],
   "source": [
    "import easydict\n",
    "from predict import predict\n",
    "\n",
    "pred_args = easydict.EasyDict({\n",
    "    'input_file':'./input.txt',\n",
    "    'output_file':'sample_pred_out.txt',\n",
    "    'model_dir':'./model',\n",
    "    'batch_size':32,\n",
    "    'no_cuda':True,\n",
    "    'predict_labels': ['CV_OCCUPATION', 'LCP_COUNTY', 'OGG_EDUCATION', 'OGG_MEDICINE','PS_NAME', 'PS_PET', 'QT_AGE', 'TMM_DISEASE', 'TMM_DRUG']\n",
    "})\n",
    "\n",
    "predict(pred_args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
